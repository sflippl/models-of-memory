{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4741ed",
   "metadata": {},
   "source": [
    "# Memory Inference Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44822a7d",
   "metadata": {},
   "source": [
    "Train on associative inference task, assess integrative encoding (proactive) vs. retrieval-time inference (reactive)\n",
    "\n",
    "Paired associate inference setup: goal is to measure A-C association\n",
    "- Simplest version: AB BC -> A?\n",
    "- Fan in & fan out versions\n",
    "\n",
    "Other task ideas (not implemented here):\n",
    "- Acquired equivalence: AB CB AD -> C?\n",
    "- Other task: AB CB XY -> A? AB AC XY -> B?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d5f85",
   "metadata": {},
   "source": [
    "QUESTIONS TO THINK THROUGH\n",
    "- task details: \n",
    "    - many targets or just one? \n",
    "    - distractors? \n",
    "    - rewards & decision probes?\n",
    "    - instructions?\n",
    "- how to get logits on output tokens\n",
    "- how to assess internal representations & dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb21a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running locally\n"
     ]
    }
   ],
   "source": [
    "import sys, random, uuid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# for running on colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    !git clone https://github.com/sflippl/models-of-memory.git\n",
    "    sys.path.append('models-of-memory')\n",
    "    dir = 'models-of-memory'\n",
    "else:\n",
    "    print(\"Running locally\")\n",
    "    dir = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ee655973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_list(model_id, exclude=['Ġ','->']):\n",
    "    \"\"\"Get all possible model tokens, exclude the ones we'll use for the experiment\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1])\n",
    "    return [token for token,idx in sorted_vocab if not any([e in token for e in exclude])]\n",
    "\n",
    "\n",
    "def generate_prompt(all_tokens, n_sets=1, n_xy_pairs=0, query_index=0, fan_in=1, fan_out=1):\n",
    "    \"\"\"\n",
    "    Each \"set\" contains AB and BC (strictly, just contains BC because of the fan in/out)\n",
    "    XY pairs are randomly intermixed if desired\n",
    "    Query index determines which of the sets is probed (\"A?\")\n",
    "    Fan_in will determine how many A tokens lead to the same B token (A1->B->C, A2->B->C)\n",
    "    Fan_out will determine how many B tokens each A token leads to (A->B1->C, A->B2->C)\n",
    "    \"\"\"\n",
    "    # ensure n_sets is a multiple of fan_out\n",
    "    if n_sets % fan_out != 0:\n",
    "        raise ValueError(\"n_sets must be a multiple of fan_out\")\n",
    "    elif fan_in > 1 and fan_out > 1:\n",
    "        raise ValueError(\"fan_in and fan_out should not both be more than 1\")\n",
    "\n",
    "    # generate tokens of different stimulus types\n",
    "    n_a_tokens = n_sets * fan_in / fan_out # if fan_in = 2, 2 A tokens for each BC; if fan_out = 2, 1 A token for 2 BCs\n",
    "    n_b_tokens = n_c_tokens = n_sets\n",
    "    n_x_tokens = n_y_tokens = n_xy_pairs\n",
    "    tokens = np.random.choice(all_tokens, n_total_tokens, replace=False)\n",
    "    split_indices = np.cumsum([n_a_tokens, n_b_tokens, n_c_tokens, n_x_tokens, n_y_tokens], dtype=int)[:-1]\n",
    "    a_tokens, b_tokens, c_tokens, x_tokens, y_tokens = np.split(tokens, split_indices)\n",
    "\n",
    "    # resolve AB pairs according to fan structure\n",
    "    ab_pairs = []\n",
    "    for set_i in range(n_sets): # For each set, determine which A(s) and B(s) participate\n",
    "        a_start = int(set_i * fan_in / fan_out) # if fanning out, want to skip through As\n",
    "        for a_idx in range(a_start, a_start + fan_in): # if fanning in, want muliple As per B\n",
    "            ab_pairs.append((a_tokens[a_idx], b_tokens[set_i]))\n",
    "\n",
    "    # BC and XY pairs are easy\n",
    "    bc_pairs = list(zip(b_tokens, c_tokens))\n",
    "    np.random.shuffle(bc_pairs)\n",
    "    xy_pairs = list(zip(x_tokens, y_tokens))\n",
    "\n",
    "    train_pairs = ab_pairs + bc_pairs # AB pairs come before BC pairs\n",
    "    # insert XY pairs at random locations\n",
    "    for xy_pair in xy_pairs:\n",
    "        train_pairs.insert(np.random.randint(0, len(train_pairs) + 1), xy_pair)\n",
    "\n",
    "    test_probe = a_tokens[query_index]\n",
    "    test_target = b_tokens[query_index] \n",
    "\n",
    "    prompt = \"Learn the associations below to make subsequent decisions.\\n\\n\"\n",
    "    for token1, token2 in train_pairs:\n",
    "        prompt += f\"{token1}->{token2} \"\n",
    "    prompt = prompt[:-1] + '\\n\\n'# take off the last space\n",
    "    prompt += f\"Probe token: {test_probe}\\n\"\n",
    "    prompt += \"Target: \"\n",
    "\n",
    "    return prompt, test_probe, test_target\n",
    "\n",
    "\n",
    "def make_pipe(model_id):\n",
    "    pipe = pipeline(\"text-generation\", model=model_id, dtype=torch.bfloat16, device_map=\"auto\")\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def query_model(pipe, inp):\n",
    "    messages = [{\"role\": \"user\", \"content\": inp}]\n",
    "    all_outputs = pipe(messages, max_new_tokens=256, temperature=0.7, top_p=0.8, top_k=20,min_p=0.1)\n",
    "    output = all_outputs[0]['generated_text'][-1]['content'].strip().lower()\n",
    "    return output, all_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d38b0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipe(model_id=\"Qwen/Qwen3-4B-Instruct-2507\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "784f81e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = get_token_list('Qwen/Qwen3-4B-Instruct-2507')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a4616806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn the associations below to make subsequent decisions.\n",
      "\n",
      "çĳª->flashdata flashdata->(',\n",
      "\n",
      "Probe token: çĳª\n",
      "Target: \n"
     ]
    }
   ],
   "source": [
    "prompt, probe, target = generate_prompt(\n",
    "    all_tokens, \n",
    "    n_sets=1, \n",
    "    n_xy_pairs=0, \n",
    "    query_index=0,\n",
    "    fan_in=1, # e.g., A1->B->C, A2->B->C\n",
    "    fan_out=1 # e.g., A->B1->C, A->B2->C\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecb572f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze how the network is doing the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "14abe494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn the associations below to make subsequent decisions.\n",
      "\n",
      "getApplication->sun å¤©åĽ½->ĉself\n",
      "\n",
      "Now learn the associated rewards.\n",
      "\n",
      "ĉself->$0 sun->$1\n",
      "\n",
      "Now decide between the following tokens, to receive associated rewards. Only output one token.\n",
      "\n",
      "å¤©åĽ½ or getApplication?\n",
      "\n",
      "Chosen token: \n"
     ]
    }
   ],
   "source": [
    "## VALUE/DECISION VERSION\n",
    "\n",
    "def generate_prompt_value(all_tokens, n_sets=1, n_xy_pairs=0, query_index=0, fan_in=1, fan_out=1):\n",
    "    \"\"\"\n",
    "    Each \"set\" contains AB and BC (strictly, just contains BC because of the fan in/out)\n",
    "    XY pairs are randomly intermixed if desired\n",
    "    Query index determines which of the sets is probed (\"A?\")\n",
    "    Fan_in will determine how many A tokens lead to the same B token (A1->B->C, A2->B->C)\n",
    "    Fan_out will determine how many B tokens each A token leads to (A->B1->C, A->B2->C)\n",
    "    \"\"\"\n",
    "    # ensure n_sets is a multiple of fan_out\n",
    "    if n_sets % fan_out != 0:\n",
    "        raise ValueError(\"n_sets must be a multiple of fan_out\")\n",
    "    elif fan_in > 1 and fan_out > 1:\n",
    "        raise ValueError(\"fan_in and fan_out should not both be more than 1\")\n",
    "\n",
    "    # in value/decision version, C tokens are rewards\n",
    "    c_tokens = np.random.permutation(np.arange(n_sets))\n",
    "    all_tokens = [a for a in all_tokens if a not in c_tokens]\n",
    "\n",
    "    # generate tokens of different stimulus types\n",
    "    n_a_tokens = n_sets * fan_in / fan_out # if fan_in = 2, 2 A tokens for each BC; if fan_out = 2, 1 A token for 2 BCs\n",
    "    n_b_tokens = n_sets\n",
    "    n_x_tokens = n_y_tokens = n_xy_pairs\n",
    "    tokens = np.random.choice(all_tokens, n_total_tokens, replace=False)\n",
    "    split_indices = np.cumsum([n_a_tokens, n_b_tokens, n_x_tokens, n_y_tokens], dtype=int)[:-1]\n",
    "    a_tokens, b_tokens, x_tokens, y_tokens = np.split(tokens, split_indices)\n",
    "    \n",
    "    # resolve AB pairs according to fan structure\n",
    "    ab_pairs = []\n",
    "    for set_i in range(n_sets): # For each set, determine which A(s) and B(s) participate\n",
    "        a_start = int(set_i * fan_in / fan_out) # if fanning out, want to skip through As\n",
    "        for a_idx in range(a_start, a_start + fan_in): # if fanning in, want muliple As per B\n",
    "            ab_pairs.append((a_tokens[a_idx], b_tokens[set_i]))\n",
    "\n",
    "    # BC and XY pairs are easy\n",
    "    c_tokens\n",
    "    bc_pairs = list(zip(b_tokens, c_tokens))\n",
    "    np.random.shuffle(bc_pairs)\n",
    "    xy_pairs = list(zip(x_tokens, y_tokens))\n",
    "\n",
    "    train_pairs = ab_pairs \n",
    "    # insert XY pairs at random locations\n",
    "    for xy_pair in xy_pairs:\n",
    "        train_pairs.insert(np.random.randint(0, len(train_pairs) + 1), xy_pair)\n",
    "\n",
    "\n",
    "    # TEST PROBE - note we are choosing the probe basically randomly here. would have to modify to be exhaustive\n",
    "    target_value = c_tokens[query_index]\n",
    "    target_a = np.random.choice([a for a, b in ab_pairs if b == b_tokens[query_index]])\n",
    "\n",
    "    foil_index = np.random.choice([i for i in range(n_sets) if i != query_index])\n",
    "    foil_a = np.random.choice([a for a, b in ab_pairs if b == b_tokens[foil_index]])\n",
    "    foil_value = c_tokens[foil_index]\n",
    "    correct_token = target_a if target_value > foil_value else foil_a\n",
    "    test_probe = (target_a, foil_a) if np.random.rand() < 0.5 else (foil_a, target_a)\n",
    "\n",
    "    prompt = \"Learn the associations below to make subsequent decisions.\\n\\n\"\n",
    "    for a,b in train_pairs:\n",
    "        prompt += f\"{a}->{b} \"\n",
    "    prompt = prompt[:-1] + \"\\n\\nNow learn the associated rewards.\\n\\n\"\n",
    "    for b,c in bc_pairs:\n",
    "        prompt += f\"{b}->${c} \"\n",
    "    prompt = prompt[:-1] + \"\\n\\nNow decide between the following tokens, to receive associated rewards. Only output one token.\\n\\n\"\n",
    "    prompt += f'{test_probe[0]} or {test_probe[1]}?\\n\\n' # take off the last space\n",
    "    prompt += \"Chosen token: \"\n",
    "\n",
    "    return prompt, test_probe, correct_token\n",
    "\n",
    "prompt, probe, target = generate_prompt_value(\n",
    "    all_tokens, \n",
    "    n_sets=2, \n",
    "    n_xy_pairs=0, \n",
    "    query_index=0,\n",
    "    fan_in=1, # e.g., A1->B->C, A2->B->C\n",
    "    fan_out=1 # e.g., A->B1->C, A->B2->C\n",
    ")\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
