{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzYfJf-gejE-"
      },
      "source": [
        "# Functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tDJcb9NhSw5",
        "outputId": "7d353c45-bea2-4083-d501-d8d9115704ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n",
            "[nltk_data] Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]   Package gazetteers is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "***Memory Inference Experiments***\n",
        "\n",
        "Train on associative inference task, assess integrative encoding (proactive) vs. retrieval-time inference (reactive)\n",
        "\n",
        "Paired associate inference setup: goal is to measure A-C association\n",
        "- Simplest version: AB BC -> A? -> measure output probability of B and C\n",
        "- Fan in & fan out versions (multiple As to one B, multiple Bs to one A)\n",
        "\n",
        "Other task ideas (not implemented here):\n",
        "- Acquired equivalence: AB CB AD -> C?\n",
        "- Other task: AB CB XY -> A? AB AC XY -> B?\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import sys, random, uuid, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('names')\n",
        "nltk.download('gazetteers')\n",
        "from nltk.corpus import wordnet, names, gazetteers\n",
        "\n",
        "# for running on colab\n",
        "if 'google.colab' in sys.modules:\n",
        "    os.system(\"git clone https://github.com/sflippl/models-of-memory.git\")\n",
        "    sys.path.append('models-of-memory')\n",
        "    dir = 'models-of-memory'\n",
        "else:\n",
        "    print(\"Running locally\")\n",
        "    dir = '.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPYmjISHewmL"
      },
      "outputs": [],
      "source": [
        "def generate_stimuli(all_tokens, n_sets, n_distractor_pairs=0,\n",
        "                     fan_in_pct=0, fan_out_pct=0, fan_in_degree=0, fan_out_degree=0,\n",
        "                     stimulus_type='words', stimulus_dict=None):\n",
        "    \"\"\"\n",
        "    Each \"set\" is A->B->C, but strictly its the number of C items (because of fan in/fan out)\n",
        "    Distractor XY pairs are randomly intermixed if requested\n",
        "    Fan structure:\n",
        "        - fan_in_pct: proportion of sets with fan-in structure (multiple As -> one B -> one C)\n",
        "        - fan_out_pct: proportion of sets with fan-out structure (one A -> multiple Bs -> one C)\n",
        "        - fan_in_degree: how many A tokens lead to the same B (e.g., 3 means A1->B, A2->B, A3->B->C)\n",
        "        - fan_out_degree: how many B tokens each A leads to (e.g., 5 means A->B1, A->B2, ..., A->B5, then A->C)\n",
        "    stimulus_type: 'words', 'names', or 'fakenames'\n",
        "    stimulus_dict: optional dict with 'A' list and 'BC_pairs' list of tuples for cities/countries mode\n",
        "    Returns:\n",
        "        - list of tuples of training pairs\n",
        "        - list of possible probes (A)\n",
        "        - list of lists of direct targets B (may be more than one for fan out)\n",
        "        - list of indirect targets C (only one per A)\n",
        "        - list of fan types (simple, fan_in, fan_out)\n",
        "    \"\"\"\n",
        "    if fan_in_pct + fan_out_pct > 1:\n",
        "        raise ValueError(\"fan_in_pct + fan_out_pct cannot exceed 1.0\")\n",
        "\n",
        "    # determine how many sets of each type\n",
        "    n_sets_fan_in = int(n_sets * fan_in_pct)\n",
        "    n_sets_fan_out = int(n_sets * fan_out_pct)\n",
        "    n_sets_simple = n_sets - n_sets_fan_in - n_sets_fan_out\n",
        "\n",
        "    # calculate and generate tokens\n",
        "    n_a_tokens = n_sets_simple + (n_sets_fan_in * fan_in_degree) + n_sets_fan_out\n",
        "    n_b_tokens = n_sets_simple + n_sets_fan_in + (n_sets_fan_out * fan_out_degree)\n",
        "    n_c_tokens = n_sets  # one C per set\n",
        "    n_x_tokens = n_y_tokens = n_distractor_pairs\n",
        "\n",
        "    if stimulus_dict:\n",
        "        a_tokens = np.random.choice(stimulus_dict['A'], n_a_tokens, replace=False)\n",
        "        # Select matching BC pairs\n",
        "        bc_indices = np.random.choice(len(stimulus_dict['BC_pairs']), n_sets, replace=False)\n",
        "        selected_bc = [stimulus_dict['BC_pairs'][i] for i in bc_indices]\n",
        "        b_tokens = [bc[0] for bc in selected_bc]\n",
        "        c_tokens = [bc[1] for bc in selected_bc]\n",
        "        x_tokens = np.random.choice(all_tokens, n_x_tokens, replace=False)\n",
        "        y_tokens = np.random.choice(all_tokens, n_y_tokens, replace=False)\n",
        "    else:\n",
        "        n_total_tokens = int(n_a_tokens + n_b_tokens + n_c_tokens + n_x_tokens + n_y_tokens)\n",
        "        tokens = np.random.choice(all_tokens, n_total_tokens, replace=False)\n",
        "        split_indices = np.cumsum([n_a_tokens, n_b_tokens, n_c_tokens, n_x_tokens, n_y_tokens], dtype=int)[:-1]\n",
        "        a_tokens, b_tokens, c_tokens, x_tokens, y_tokens = np.split(tokens, split_indices)\n",
        "\n",
        "    # build pairs and track mappings\n",
        "    ab_pairs, bc_pairs, xy_pairs = [],[],[]\n",
        "    direct_targets = []  # list of lists: each entry corresponds to each A, containing a list of its Bs\n",
        "    indirect_targets = []  # list: each entry corresponds to each A, containing its C\n",
        "    pair_types = [] # track whether each pair is AB, BC, or XY\n",
        "    fan_types = []  # track whether each A is simple, fan_in, or fan_out\n",
        "    a_idx, b_idx, c_idx = 0, 0, 0 # keep track of where we are in the tokens\n",
        "\n",
        "    # simple sets: 1 A -> 1 B -> 1 C\n",
        "    for _ in range(n_sets_simple):\n",
        "        ab_pairs.append((a_tokens[a_idx], b_tokens[b_idx]))\n",
        "        bc_pairs.append((b_tokens[b_idx], c_tokens[c_idx]))\n",
        "        direct_targets.append([b_tokens[b_idx]])\n",
        "        indirect_targets.append(c_tokens[c_idx])\n",
        "        fan_types.append('simple')\n",
        "        a_idx += 1\n",
        "        b_idx += 1\n",
        "        c_idx += 1\n",
        "\n",
        "    # Fan-in sets: multiple As -> 1 B -> 1 C\n",
        "    for _ in range(n_sets_fan_in):\n",
        "        for _ in range(fan_in_degree): # A1->B, A2->B, ...\n",
        "            ab_pairs.append((a_tokens[a_idx], b_tokens[b_idx]))\n",
        "            direct_targets.append([b_tokens[b_idx]])\n",
        "            indirect_targets.append(c_tokens[c_idx])\n",
        "            fan_types.append('fan_in')\n",
        "            a_idx += 1\n",
        "        bc_pairs.append((b_tokens[b_idx], c_tokens[c_idx])) # only one BC pair per set\n",
        "        b_idx += 1\n",
        "        c_idx += 1\n",
        "\n",
        "    # Fan-out sets: 1 A -> multiple Bs; only first B -> C\n",
        "    for _ in range(n_sets_fan_out):\n",
        "        b_list = []\n",
        "        for i in range(fan_out_degree): # A->B1, A->B2, ...\n",
        "            ab_pairs.append((a_tokens[a_idx], b_tokens[b_idx]))\n",
        "            b_list.append(b_tokens[b_idx])\n",
        "            if i == 0:  # Only pair the first B with C\n",
        "                bc_pairs.append((b_tokens[b_idx], c_tokens[c_idx]))\n",
        "            b_idx += 1\n",
        "        direct_targets.append(b_list)\n",
        "        indirect_targets.append(c_tokens[c_idx])\n",
        "        fan_types.append('fan_out')\n",
        "        a_idx += 1\n",
        "        c_idx += 1\n",
        "\n",
        "    xy_pairs = list(zip(x_tokens, y_tokens))\n",
        "\n",
        "    # Shuffle BC pairs\n",
        "    bc_perm = np.random.permutation(len(bc_pairs))\n",
        "    bc_shuffled = [bc_pairs[i] for i in bc_perm]\n",
        "\n",
        "    # Combine training pairs and track types\n",
        "    train_pairs = ab_pairs + bc_shuffled\n",
        "    pair_types = ['AB'] * len(ab_pairs) + ['BC'] * len(bc_pairs)\n",
        "\n",
        "    # Insert XY pairs at random locations\n",
        "    for xy_pair in xy_pairs:\n",
        "        idx = np.random.randint(0, len(train_pairs) + 1)\n",
        "        train_pairs.insert(idx, xy_pair)\n",
        "        pair_types.insert(idx, 'XY')\n",
        "\n",
        "    return train_pairs, pair_types, fan_types, a_tokens, direct_targets, indirect_targets\n",
        "\n",
        "\n",
        "\n",
        "def generate_prompt(train_pairs, pair_types, test_probe, stimulus_type='words',\n",
        "                    prompt_type='standard', target=None, foil=None, ):\n",
        "    prompt = \"\"\n",
        "    # training pairs\n",
        "    if stimulus_type == 'words':\n",
        "        for p in train_pairs:\n",
        "            prompt += f\"{p[0]}->{p[1]} \"\n",
        "    else:\n",
        "        for p, p_type in zip(train_pairs, pair_types):\n",
        "            if p_type == 'AB' or ():\n",
        "                prompt += f\"{p[0]} is from {p[1]}. \"\n",
        "            elif p_type == 'BC':\n",
        "                prompt += f\"{p[0]} is in {p[1]}. \"\n",
        "            elif random.random() < 0.5: # XY\n",
        "                prompt += f\"{p[0]} is from {p[1]}. \"\n",
        "            else:\n",
        "                prompt += f\"{p[0]} is in {p[1]}. \"\n",
        "\n",
        "    # Query structure\n",
        "    if prompt_type == 'standard':\n",
        "        if stimulus_type == 'words':\n",
        "            prompt += f\"{test_probe}->\"\n",
        "        else:\n",
        "            prompt += f\"{test_probe} is from \"\n",
        "    elif prompt_type == 'afc':\n",
        "        choices = [target, foil]\n",
        "        random.shuffle(choices)\n",
        "        if stimulus_type == 'words':\n",
        "            prompt += f\"{test_probe}->{choices[0]} or {choices[1]}? \"\n",
        "        else:\n",
        "            prompt += f\"Is {test_probe} from {choices[0]} or {choices[1]}? \"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_uAeu6Nexdx"
      },
      "outputs": [],
      "source": [
        "def load_model(model_id):\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=\"auto\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def get_single_tokens(tokenizer, arr):\n",
        "  return [a for a in arr if len( tokenizer(a, add_special_tokens=False)[\"input_ids\"] ) == 1 ]\n",
        "\n",
        "def get_single_token_nouns(tokenizer):\n",
        "    \"\"\"get list of nouns from wordnet that are single tokens in the given model\"\"\"\n",
        "    nouns = [lemma.name() for syn in wordnet.all_synsets(\"n\") for lemma in syn.lemmas()]\n",
        "    nouns = [n for n in nouns if n.isalpha()]\n",
        "    return get_single_tokens(tokenizer, nouns)\n",
        "\n",
        "def get_single_token_names(tokenizer):\n",
        "    \"\"\"get list of names from nltk names corpus that are single tokens\"\"\"\n",
        "    all_names = names.words('male.txt') + names.words('female.txt')\n",
        "    return list(set(get_single_tokens(tokenizer, all_names)))\n",
        "\n",
        "def get_single_token_geography(tokenizer):\n",
        "    pairs = [\n",
        "        (\"Paris\", \"France\"), (\"Berlin\", \"Germany\"), (\"Rome\", \"Italy\"), (\"Madrid\", \"Spain\"), (\"Lisbon\", \"Portugal\"),\n",
        "        (\"Vienna\", \"Austria\"), (\"Brussels\", \"Belgium\"), (\"Athens\", \"Greece\"), (\"Warsaw\", \"Poland\"), (\"Prague\", \"Czechia\"),\n",
        "        (\"Tokyo\", \"Japan\"), (\"Seoul\", \"Korea\"), (\"Beijing\", \"China\"), (\"Bangkok\", \"Thailand\"), (\"Hanoi\", \"Vietnam\"),\n",
        "        (\"Delhi\", \"India\"), (\"Manila\", \"Philippines\"), (\"Jakarta\", \"Indonesia\"), (\"Cairo\", \"Egypt\"), (\"Nairobi\", \"Kenya\"),\n",
        "        (\"Lagos\", \"Nigeria\"), (\"Accra\", \"Ghana\"), (\"Tunis\", \"Tunisia\"), (\"Algiers\", \"Algeria\"), (\"Moscow\", \"Russia\"),\n",
        "        (\"Kyiv\", \"Ukraine\"), (\"Oslo\", \"Norway\"), (\"Stockholm\", \"Sweden\"), (\"Helsinki\", \"Finland\"), (\"Copenhagen\", \"Denmark\"),\n",
        "        (\"Dublin\", \"Ireland\"), (\"London\", \"UK\"), (\"Ottawa\", \"Canada\"), (\"Mexico\", \"Mexico\"), (\"Havana\", \"Cuba\"),\n",
        "        (\"Kingston\", \"Jamaica\"), (\"Panama\", \"Panama\"), (\"Bogota\", \"Colombia\"), (\"Quito\", \"Ecuador\"), (\"Lima\", \"Peru\"),\n",
        "        (\"Santiago\", \"Chile\"), (\"Brasilia\", \"Brazil\"), (\"Caracas\", \"Venezuela\"), (\"Sydney\", \"Australia\"), (\"Suva\", \"Fiji\"),\n",
        "        (\"Amman\", \"Jordan\"), (\"Beirut\", \"Lebanon\"), (\"Baghdad\", \"Iraq\"), (\"Tehran\", \"Iran\"), (\"Riyadh\", \"Saudi\"),\n",
        "        (\"Kuwait\", \"Kuwait\"), (\"Doha\", \"Qatar\"), (\"Muscat\", \"Oman\"), (\"Kabul\", \"Afghanistan\"), (\"Islamabad\", \"Pakistan\")\n",
        "    ]\n",
        "    single_token_pairs = []\n",
        "    for city, country in pairs:\n",
        "        country_ids = tokenizer(country, add_special_tokens=False)[\"input_ids\"] # this is the only one that needs to be single-token\n",
        "        if len(country_ids) == 1:\n",
        "            single_token_pairs.append((city, country))\n",
        "    return single_token_pairs\n",
        "\n",
        "\n",
        "def get_fictional_single_tokens(tokenizer, count):\n",
        "    \"\"\"\n",
        "    Scavenges the model's own vocabulary for strings it treats as atomic\n",
        "    but that are not recognized as real English nouns.\n",
        "    \"\"\"\n",
        "    # Get all potential strings from the model's vocabulary\n",
        "    # (Handling various tokenizer formats like GPT, Llama, and BERT)\n",
        "    vocab = tokenizer.get_vocab().keys()\n",
        "    fictional_pool = []\n",
        "\n",
        "    for t in vocab:\n",
        "        # 1. Clean up subword/whitespace markers (like 'Ġ', ' ', '##')\n",
        "        clean = t.replace('Ġ', '').replace(' ', '').replace('##', '')\n",
        "\n",
        "        # 2. Heuristics for a \"Country\" name (alphabetic, 5-8 chars)\n",
        "        if clean.isalpha() and 5 <= len(clean) <= 8:\n",
        "            # 3. Validation: Verify it stays a single token and isn't a real word\n",
        "            if len(tokenizer.encode(clean, add_special_tokens=False)) == 1:\n",
        "                if not wordnet.synsets(clean.lower()):\n",
        "                    fictional_pool.append(clean.capitalize())\n",
        "\n",
        "        if len(fictional_pool) >= count + 50: # Get a buffer then stop\n",
        "            break\n",
        "\n",
        "    return random.sample(fictional_pool, min(count, len(fictional_pool)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAt4mLY0qjrw"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b3ce5baba9024a308a2f495e538ed96e",
            "9127bb2448174091a67100fc1b8db409",
            "1bd894a160f14966a431c3520af9d769",
            "ea7ab3aa33fe4a0b948cbdfb0f12cc01",
            "34bf0ab2f378408ca6042053211df509",
            "0cbd60227b0943af86cc76de7919005a",
            "28b52ab6e06a41c0b9815ba76d065fb7",
            "a57583b232be43b79bd051b7f2e51ce8",
            "6b6f361f6d22460fb51a599bb7d3f10e",
            "435f70dbfe4049caba4824965f661b40",
            "e41be5f9cc0841c8a6510af028325518"
          ]
        },
        "id": "a2b8JI_tqkYs",
        "outputId": "a858e87b-b8e4-40f3-e48e-6fa8bd47d8f4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3ce5baba9024a308a2f495e538ed96e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_id = \"Qwen/Qwen3-4B\"\n",
        "model, tokenizer = load_model(model_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqOObg0leqkF"
      },
      "source": [
        "# Run (words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZko8Sxejygf"
      },
      "outputs": [],
      "source": [
        "n_sets = 3 # how many C items (BC pairs)\n",
        "n_distractor_pairs = 0 # how many XY pairs\n",
        "fan_in_pct = 0.0 # percentage of sets that fan in\n",
        "fan_out_pct = 0.0 # percentage of sets that fan out\n",
        "fan_in_degree = 2 # how many As per B\n",
        "fan_out_degree = 2 # how many Bs per A\n",
        "\n",
        "stimulus_type = 'words'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biGsOB3ZxJqw"
      },
      "outputs": [],
      "source": [
        "all_tokens = get_single_token_nouns(tokenizer)\n",
        "\n",
        "training_pairs, pair_types, fan_types, test_probes, test_direct_targets, test_indirect_targets = generate_stimuli(\n",
        "    all_tokens, n_sets, stimulus_type=stimulus_type)\n",
        "\n",
        "probe_idx = 0\n",
        "probe = test_probes[probe_idx]\n",
        "direct_targets = test_direct_targets[probe_idx]\n",
        "indirect_target = test_indirect_targets[probe_idx]\n",
        "fan_type = fan_types[probe_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YSnkgOGxd2v",
        "outputId": "c65b5904-11d5-483b-e59e-604958d5dbc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scene->dog box->being oka->progress being->lug progress->comfort dog->Nation scene-> \n",
            "\n",
            "Probe: scene, direct target: dog, indirect target: Nation\n",
            "\n",
            "Results (ranked):\n",
            "(np.str_('dog'), {'logit': 13.5625, 'prob': 0.05419921875, 'rank': 2})\n",
            "(np.str_('scene'), {'logit': 12.6875, 'prob': 0.0225830078125, 'rank': 3})\n",
            "(np.str_('being'), {'logit': 11.5, 'prob': 0.00689697265625, 'rank': 7})\n",
            "(np.str_('Nation'), {'logit': 10.875, 'prob': 0.003692626953125, 'rank': 16})\n",
            "(np.str_('progress'), {'logit': 10.125, 'prob': 0.00174713134765625, 'rank': 47})\n",
            "(np.str_('box'), {'logit': 9.9375, 'prob': 0.00144195556640625, 'rank': 56})\n",
            "(np.str_('comfort'), {'logit': 9.375, 'prob': 0.000823974609375, 'rank': 120})\n",
            "(np.str_('lug'), {'logit': 6.59375, 'prob': 5.1021575927734375e-05, 'rank': 2177})\n",
            "(np.str_('oka'), {'logit': 4.21875, 'prob': 4.738569259643555e-06, 'rank': 12634})\n"
          ]
        }
      ],
      "source": [
        "# STANDARD PROBE\n",
        "prompt_type = 'standard'\n",
        "prompt = generate_prompt(training_pairs, pair_types, probe, stimulus_type=stimulus_type,\n",
        "                         prompt_type = prompt_type)\n",
        "\n",
        "print(prompt, '\\n')\n",
        "print(f'Probe: {probe}, direct target: {direct_targets[0]}, indirect target: {indirect_target}\\n')\n",
        "\n",
        "results = query_model(model, tokenizer, prompt,\n",
        "                      list(set([a for a,b in training_pairs] + [b for a,b in training_pairs])))\n",
        "\n",
        "\n",
        "# print results, sorted by the \"rank\" value of the value of each key\n",
        "sorted_results = sorted(results.items(), key=lambda item: item[1]['rank'])\n",
        "print('Results (ranked):')\n",
        "_ = [print(i) for i in sorted_results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-eoXIYyjirt",
        "outputId": "341fb8b1-9cbc-4734-e78a-0d0deb9db3c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "scene->dog box->being oka->progress being->lug progress->comfort dog->Nation scene->comfort or Nation?  \n",
            "\n",
            "Probe: scene, direct target: dog, indirect target: Nation\n",
            "\n",
            "Results (ranked):\n",
            "(np.str_('scene'), {'logit': 5.875, 'prob': 4.842877388000488e-07, 'rank': 16137})\n",
            "(np.str_('comfort'), {'logit': 5.09375, 'prob': 2.2165477275848389e-07, 'rank': 24362})\n",
            "(np.str_('progress'), {'logit': 4.1875, 'prob': 8.987262845039368e-08, 'rank': 37816})\n",
            "(np.str_('lug'), {'logit': 2.59375, 'prob': 1.8277205526828766e-08, 'rank': 74465})\n",
            "(np.str_('dog'), {'logit': 1.96875, 'prob': 9.778887033462524e-09, 'rank': 93759})\n",
            "(np.str_('oka'), {'logit': 1.515625, 'prob': 6.199115887284279e-09, 'rank': 105245})\n",
            "(np.str_('being'), {'logit': 1.484375, 'prob': 5.995389074087143e-09, 'rank': 106016})\n",
            "(np.str_('Nation'), {'logit': 0.97265625, 'prob': 3.5943230614066124e-09, 'rank': 117660})\n",
            "(np.str_('box'), {'logit': -0.3125, 'prob': 9.968061931431293e-10, 'rank': 138842})\n"
          ]
        }
      ],
      "source": [
        "# AFC PROBE\n",
        "prompt_type = 'afc'\n",
        "foil = None\n",
        "if prompt_type == 'afc':\n",
        "    other_targets = [t for t in test_indirect_targets if t != indirect_target]\n",
        "    foil = random.choice(other_targets)\n",
        "\n",
        "prompt = generate_prompt(training_pairs, pair_types, probe, stimulus_type=stimulus_type,\n",
        "                         prompt_type = prompt_type, target=indirect_target, foil=foil)\n",
        "\n",
        "print(prompt, '\\n')\n",
        "print(f'Probe: {probe}, direct target: {direct_targets[0]}, indirect target: {indirect_target}\\n')\n",
        "\n",
        "results = query_model(model, tokenizer, prompt,\n",
        "                      list(set([a for a,b in training_pairs] + [b for a,b in training_pairs])))\n",
        "\n",
        "\n",
        "# print results, sorted by the \"rank\" value of the value of each key\n",
        "sorted_results = sorted(results.items(), key=lambda item: item[1]['rank'])\n",
        "print('Results (ranked):')\n",
        "_ = [print(i) for i in sorted_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9q33TRSrrL3"
      },
      "source": [
        "# Run (names/cities/countries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SshNrkVrnbO"
      },
      "outputs": [],
      "source": [
        "n_sets=3\n",
        "stimulus_type = 'names'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7NlZkr_rzos"
      },
      "outputs": [],
      "source": [
        "names_list = get_single_token_names(tokenizer)\n",
        "geo_pairs = get_single_token_geography(tokenizer)\n",
        "stimulus_dict = {'A': names_list,'BC_pairs': geo_pairs}\n",
        "\n",
        "training_pairs, pair_types, fan_types, test_probes, test_direct_targets, test_indirect_targets = generate_stimuli(\n",
        "    all_tokens=[], n_sets=n_sets, stimulus_type='names', stimulus_dict=stimulus_dict)\n",
        "\n",
        "probe_idx = 0\n",
        "probe = test_probes[probe_idx]\n",
        "direct_targets = test_direct_targets[probe_idx]\n",
        "indirect_target = test_indirect_targets[probe_idx]\n",
        "fan_type = fan_types[probe_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbXiTooQxzqd",
        "outputId": "f509830d-40b1-40d8-a32c-99aa93def007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tam is from Delhi. Mitch is from Rome. Chelsea is from Riyadh. Riyadh is in Saudi. Delhi is in India. Rome is in Italy. Tam is from  \n",
            "\n",
            "Probe: Tam, direct target: Delhi, indirect target: India\n",
            "\n",
            "Results (ranked):\n",
            "('India', {'logit': 12.9375, 'prob': 0.0002918243408203125, 'rank': 77})\n",
            "('Saudi', {'logit': 9.6875, 'prob': 1.1324882507324219e-05, 'rank': 506})\n",
            "(np.str_('Chelsea'), {'logit': 8.75, 'prob': 4.4405460357666016e-06, 'rank': 924})\n",
            "('Italy', {'logit': 8.5, 'prob': 3.4570693969726562e-06, 'rank': 1076})\n",
            "('Delhi', {'logit': 7.90625, 'prob': 1.9073486328125e-06, 'rank': 1633})\n",
            "(np.str_('Tam'), {'logit': 5.5625, 'prob': 1.8347054719924927e-07, 'rank': 7960})\n",
            "(np.str_('Mitch'), {'logit': 1.6953125, 'prob': 3.841705620288849e-09, 'rank': 63256})\n",
            "('Riyadh', {'logit': 1.0546875, 'prob': 2.0227162167429924e-09, 'rank': 81347})\n",
            "('Rome', {'logit': 1.0546875, 'prob': 2.0227162167429924e-09, 'rank': 81347})\n"
          ]
        }
      ],
      "source": [
        "# STANDARD PROBE\n",
        "prompt_type = 'standard'\n",
        "prompt = generate_prompt(training_pairs, pair_types, probe, stimulus_type=stimulus_type,\n",
        "                         prompt_type = prompt_type)\n",
        "\n",
        "print(prompt, '\\n')\n",
        "print(f'Probe: {probe}, direct target: {direct_targets[0]}, indirect target: {indirect_target}\\n')\n",
        "\n",
        "results = query_model(model, tokenizer, prompt,\n",
        "                      list(set([a for a,b in training_pairs] + [b for a,b in training_pairs])))\n",
        "\n",
        "\n",
        "# print results, sorted by the \"rank\" value of the value of each key\n",
        "sorted_results = sorted(results.items(), key=lambda item: item[1]['rank'])\n",
        "print('Results (ranked):')\n",
        "_ = [print(i) for i in sorted_results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQpduqD2x1cY",
        "outputId": "e59b8359-8765-40d5-93e9-f0cd391c849d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tam is from Delhi. Mitch is from Rome. Chelsea is from Riyadh. Riyadh is in Saudi. Delhi is in India. Rome is in Italy. Is Tam from India or Saudi?  \n",
            "\n",
            "Probe: Tam, direct target: Delhi, indirect target: India\n",
            "\n",
            "Results (ranked):\n",
            "(np.str_('Tam'), {'logit': 11.4375, 'prob': 1.4722347259521484e-05, 'rank': 315})\n",
            "(np.str_('Chelsea'), {'logit': 9.8125, 'prob': 2.8908252716064453e-06, 'rank': 779})\n",
            "('India', {'logit': 6.15625, 'prob': 7.497146725654602e-08, 'rank': 5940})\n",
            "('Saudi', {'logit': 4.875, 'prob': 2.0721927285194397e-08, 'rank': 11285})\n",
            "('Italy', {'logit': 3.015625, 'prob': 3.230525180697441e-09, 'rank': 26658})\n",
            "(np.str_('Mitch'), {'logit': 2.203125, 'prob': 1.433363649994135e-09, 'rank': 37331})\n",
            "('Delhi', {'logit': 2.15625, 'prob': 1.367880031466484e-09, 'rank': 37995})\n",
            "('Riyadh', {'logit': -0.98828125, 'prob': 5.911715561524034e-11, 'rank': 103650})\n",
            "('Rome', {'logit': -0.98828125, 'prob': 5.911715561524034e-11, 'rank': 103650})\n"
          ]
        }
      ],
      "source": [
        "# AFC PROBE\n",
        "prompt_type = 'afc'\n",
        "foil = None\n",
        "if prompt_type == 'afc':\n",
        "    other_targets = [t for t in test_indirect_targets if t != indirect_target]\n",
        "    foil = random.choice(other_targets)\n",
        "\n",
        "prompt = generate_prompt(training_pairs, pair_types, probe, stimulus_type=stimulus_type,\n",
        "                         prompt_type = prompt_type, target=indirect_target, foil=foil)\n",
        "\n",
        "print(prompt, '\\n')\n",
        "print(f'Probe: {probe}, direct target: {direct_targets[0]}, indirect target: {indirect_target}\\n')\n",
        "\n",
        "results = query_model(model, tokenizer, prompt,\n",
        "                      list(set([a for a,b in training_pairs] + [b for a,b in training_pairs])))\n",
        "\n",
        "\n",
        "# print results, sorted by the \"rank\" value of the value of each key\n",
        "sorted_results = sorted(results.items(), key=lambda item: item[1]['rank'])\n",
        "print('Results (ranked):')\n",
        "_ = [print(i) for i in sorted_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiAzMseHtQ5x"
      },
      "source": [
        "# Run (fake geography)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnYt1i4XtUbU"
      },
      "outputs": [],
      "source": [
        "n_sets=3\n",
        "stimulus_type = 'fakenames'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdo_jQQNtWt2"
      },
      "outputs": [],
      "source": [
        "names_list = get_single_token_names(tokenizer)\n",
        "fake_geo = get_fictional_single_tokens(tokenizer, 100)\n",
        "geo_pairs = list(zip(fake_geo[0:50], fake_geo[50:100]))\n",
        "stimulus_dict = {'A': names_list,'BC_pairs': geo_pairs}\n",
        "\n",
        "training_pairs, pair_types, fan_types, test_probes, test_direct_targets, test_indirect_targets = generate_stimuli(\n",
        "    all_tokens=[], n_sets=n_sets, stimulus_type='names', stimulus_dict=stimulus_dict)\n",
        "\n",
        "probe_idx = 0\n",
        "probe = test_probes[probe_idx]\n",
        "direct_targets = test_direct_targets[probe_idx]\n",
        "indirect_target = test_indirect_targets[probe_idx]\n",
        "fan_type = fan_types[probe_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lexk0WkMycmG",
        "outputId": "f510747a-58f1-4c9e-b2e8-0b2bd4a8ca47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diamond is from Abble. Tab is from Przedsi. Jose is from Esting. Abble is in Uracy. Esting is in Indrical. Przedsi is in Duction. Diamond is from  \n",
            "\n",
            "Probe: Diamond, direct target: Abble, indirect target: Uracy\n",
            "\n",
            "Results (ranked):\n",
            "('Abble', {'logit': 9.6875, 'prob': 1.9550323486328125e-05, 'rank': 586})\n",
            "('Uracy', {'logit': 9.625, 'prob': 1.8358230590820312e-05, 'rank': 623})\n",
            "('Indrical', {'logit': 5.8125, 'prob': 4.0605664253234863e-07, 'rank': 11414})\n",
            "(np.str_('Jose'), {'logit': 4.375, 'prob': 9.639188647270203e-08, 'rank': 28131})\n",
            "(np.str_('Diamond'), {'logit': 4.28125, 'prob': 8.800998330116272e-08, 'rank': 29630})\n",
            "('Esting', {'logit': 4.1875, 'prob': 8.009374141693115e-08, 'rank': 31307})\n",
            "('Przedsi', {'logit': 3.78125, 'prob': 5.3318217396736145e-08, 'rank': 39199})\n",
            "(np.str_('Tab'), {'logit': 3.59375, 'prob': 4.423782229423523e-08, 'rank': 43110})\n",
            "('Duction', {'logit': 2.65625, 'prob': 1.7345882952213287e-08, 'rank': 66046})\n"
          ]
        }
      ],
      "source": [
        "# STANDARD PROBE\n",
        "prompt_type = 'standard'\n",
        "prompt = generate_prompt(training_pairs, pair_types, probe, stimulus_type=stimulus_type,\n",
        "                         prompt_type = prompt_type)\n",
        "\n",
        "print(prompt, '\\n')\n",
        "print(f'Probe: {probe}, direct target: {direct_targets[0]}, indirect target: {indirect_target}\\n')\n",
        "\n",
        "results = query_model(model, tokenizer, prompt,\n",
        "                      list(set([a for a,b in training_pairs] + [b for a,b in training_pairs])))\n",
        "\n",
        "\n",
        "# print results, sorted by the \"rank\" value of the value of each key\n",
        "sorted_results = sorted(results.items(), key=lambda item: item[1]['rank'])\n",
        "print('Results (ranked):')\n",
        "_ = [print(i) for i in sorted_results]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMSqRTydydiY",
        "outputId": "95a52f9f-ff43-4d2a-b3d5-cbdad4e0111d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diamond is from Abble. Tab is from Przedsi. Jose is from Esting. Abble is in Uracy. Esting is in Indrical. Przedsi is in Duction. Is Diamond from Duction or Uracy?  \n",
            "\n",
            "Probe: Diamond, direct target: Abble, indirect target: Uracy\n",
            "\n",
            "Results (ranked):\n",
            "(np.str_('Diamond'), {'logit': 8.9375, 'prob': 5.62518835067749e-07, 'rank': 2699})\n",
            "('Abble', {'logit': 3.4375, 'prob': 2.2992026060819626e-09, 'rank': 41382})\n",
            "('Indrical', {'logit': 2.65625, 'prob': 1.0550138540565968e-09, 'rank': 54691})\n",
            "('Uracy', {'logit': 2.015625, 'prob': 5.529727786779404e-10, 'rank': 67484})\n",
            "(np.str_('Jose'), {'logit': 1.8984375, 'prob': 4.94765117764473e-10, 'rank': 70037})\n",
            "('Duction', {'logit': 1.453125, 'prob': 3.1650415621697903e-10, 'rank': 79832})\n",
            "('Przedsi', {'logit': -0.5859375, 'prob': 4.1154635255225e-11, 'rank': 122009})\n",
            "(np.str_('Tab'), {'logit': -1.375, 'prob': 1.864464138634503e-11, 'rank': 133103})\n",
            "('Esting', {'logit': -2.359375, 'prob': 6.963318810448982e-12, 'rank': 142367})\n"
          ]
        }
      ],
      "source": [
        "# AFC PROBE\n",
        "prompt_type = 'afc'\n",
        "foil = None\n",
        "if prompt_type == 'afc':\n",
        "    other_targets = [t for t in test_indirect_targets if t != indirect_target]\n",
        "    foil = random.choice(other_targets)\n",
        "\n",
        "prompt = generate_prompt(training_pairs, pair_types, probe, stimulus_type=stimulus_type,\n",
        "                         prompt_type = prompt_type, target=indirect_target, foil=foil)\n",
        "\n",
        "print(prompt, '\\n')\n",
        "print(f'Probe: {probe}, direct target: {direct_targets[0]}, indirect target: {indirect_target}\\n')\n",
        "\n",
        "results = query_model(model, tokenizer, prompt,\n",
        "                      list(set([a for a,b in training_pairs] + [b for a,b in training_pairs])))\n",
        "\n",
        "\n",
        "# print results, sorted by the \"rank\" value of the value of each key\n",
        "sorted_results = sorted(results.items(), key=lambda item: item[1]['rank'])\n",
        "print('Results (ranked):')\n",
        "_ = [print(i) for i in sorted_results]"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "PzYfJf-gejE-",
        "TAt4mLY0qjrw"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cbd60227b0943af86cc76de7919005a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bd894a160f14966a431c3520af9d769": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a57583b232be43b79bd051b7f2e51ce8",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b6f361f6d22460fb51a599bb7d3f10e",
            "value": 3
          }
        },
        "28b52ab6e06a41c0b9815ba76d065fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34bf0ab2f378408ca6042053211df509": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "435f70dbfe4049caba4824965f661b40": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b6f361f6d22460fb51a599bb7d3f10e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9127bb2448174091a67100fc1b8db409": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cbd60227b0943af86cc76de7919005a",
            "placeholder": "​",
            "style": "IPY_MODEL_28b52ab6e06a41c0b9815ba76d065fb7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "a57583b232be43b79bd051b7f2e51ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3ce5baba9024a308a2f495e538ed96e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9127bb2448174091a67100fc1b8db409",
              "IPY_MODEL_1bd894a160f14966a431c3520af9d769",
              "IPY_MODEL_ea7ab3aa33fe4a0b948cbdfb0f12cc01"
            ],
            "layout": "IPY_MODEL_34bf0ab2f378408ca6042053211df509"
          }
        },
        "e41be5f9cc0841c8a6510af028325518": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea7ab3aa33fe4a0b948cbdfb0f12cc01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_435f70dbfe4049caba4824965f661b40",
            "placeholder": "​",
            "style": "IPY_MODEL_e41be5f9cc0841c8a6510af028325518",
            "value": " 3/3 [00:49&lt;00:00, 13.32s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
